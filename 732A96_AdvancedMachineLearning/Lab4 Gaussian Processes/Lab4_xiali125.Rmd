---
title: "LAB 4: GAUSSIAN PROCESSES"  
author: 
  - xiali125@student.liu.se
date: "2025-10-8"
output:
  pdf_document: 
    latex_engine: xelatex
    number_sections: false
    fig_width: 7
    highlight: tango
    toc: true
editor_options: 
  markdown: 
    wrap: 72
---

The purpose of the lab is to put in practice some of the concepts
covered in the lectures.

# 1. Implementing GP Regression

This first exercise will have you writing your own code for the Gaussian
process regression model:

$$
y = f(x) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma_n^2), \quad f \sim \mathcal{GP}(0, k(x, x'))
$$

You must implement Algorithm 2.1 on page 19 of *Rasmussen and Williams’*
book. The algorithm uses the **Cholesky decomposition** (`chol` in R) to
attain numerical stability. Note that $L$ in the algorithm is a lower
triangular matrix, whereas the R function returns an upper triangular
matrix. So, you need to transpose the output of the R function. In the
algorithm, the notation $A/b$ means the vector $x$ that solves the
equation $A x = b$ (see p. xvii in the book). This is implemented in R
with the help of the function `solve`.

## (1) Implement the posterior simulation

Write your own code for simulating from the posterior distribution of
$f$ using the squared exponential kernel. The function (name it
`posteriorGP`) should return a vector with the posterior mean and
variance of $f$, both evaluated at a set of $x$-values ($X^*$). You can
assume that the prior mean of $f$ is zero for all $x$.

The function should have the following inputs:

-   `X`: Vector of training inputs.
-   `y`: Vector of training targets/outputs.
-   `XStar`: Vector of inputs where the posterior distribution is
    evaluated, i.e. $X^*$.
-   `sigmaNoise`: Noise standard deviation $\sigma_n$.
-   `k`: Covariance function or kernel. That is, the kernel should be a
    separate function (see the file `GaussianProcesses.R` on the course
    web page).
    
```{r eval=TRUE}
SquaredExpKernel <- function(x1, x2, sigmaF = 1, l = 1) {
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA, n1, n2)
  for (i in 1:n2) {
    K[, i] <- sigmaF^2 * exp(-0.5 * ((x1 - x2[i]) / l)^2)
  }
  return(K)
}

posteriorGP <- function (x, y, xstar, sig, k, ...) {

  K <- k(x, x, ...)
  Kstar <- k(x, xstar, ...)
  Kss <- k(xstar, xstar, ...)
  # predictive mean
  L <- chol(K + sig^2 * diag(nrow(K)))  # add noise
  # L in the algorithm is a lower triangular matrix,
  # whereas the R function returns an upper triangular matrix
  L <- t(L)
  
  alpha <- solve(t(L), solve(L, y))
  mean <- t(Kstar) %*% alpha
  
  # predictive var
  v <- solve (L, Kstar)
  var <-  Kss - t(v) %*% v
  
  res = list(mean = mean, var = var)
  return(res)
}
```
    

## (2) Single observation update

Let the prior hyperparameters be $\sigma_f = 1$ and $\ell = 0.3$. Update
this prior with a single observation $(x, y) = (0.4, 0.719)$. Assume
that $\sigma_n = 0.1$.

Plot the posterior mean of $f$ over the interval $x \in [-1, 1]$. Plot
also 95% probability (pointwise) bands for $f$.

```{r echo=FALSE}
sigmaF <- 1
l <- 0.3

library(tibble)
library(ggplot2)
#tibble
x_values <- seq(-1, 1, length.out = 5000)
mean_pred  <- posteriorGP(0.4, 0.719, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred <- posteriorGP(0.4, 0.719, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data <- tibble(
  x_values = x_values,
  y_values = mean_pred,
  sd = sqrt(diag(var_pred)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = 0.4,
    y = 0.719,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (One Training Point)", x = "x", y = "y")

```


## (3) Second observation update

Update your posterior from (2) with another observation
$(x, y) = (-0.6, -0.044)$. Plot the posterior mean of $f$ over the
interval $x \in [-1, 1]$. Plot also 95% probability (pointwise) bands
for $f$.

*Hint:* Updating the posterior after one observation with a new
observation gives the same result as updating the prior directly with
the two observations.

```{r echo=FALSE}
obs_x <- c(0.4, -0.6)
obs_y <- c(0.719, -0.044)

#tibble
mean_pred2  <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred2 <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data2 <- tibble(
  x_values = x_values,
  y_values = mean_pred2,
  sd = sqrt(diag(var_pred2)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data2, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = obs_x,
    y = obs_y,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (Two Training Point)", x = "x", y = "y")
```


## (4) Full dataset posterior

Compute the posterior distribution of $f$ using all five data points in
the table below (note that the two previous observations are included in
the table). Plot the posterior mean of $f$ over the interval
$x \in [-1, 1]$. Plot also 95% probability (pointwise) bands for $f$.

```         
| $x$ | -1.0  |  -0.6  |  -0.2  |  0.4  |  0.8   |
|:---:|:-----:|:------:|:------:|:-----:|:------:|
| $y$ | 0.768 | -0.044 | -0.940 | 0.719 | -0.664 |
```
```{r echo=FALSE}
obs_x <- c(-1, -0.6, -0.2, 0.4, 0.8)
obs_y <- c(0.768, -0.044, -0.940, 0.719, -0.664)
mean_pred3  <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred3 <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data3 <- tibble(
  x_values = x_values,
  y_values = mean_pred3,
  sd = sqrt(diag(var_pred3)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data3, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = obs_x,
    y = obs_y,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (Five Training Point)", x = "x", y = "y")
```

## (5) Hyperparameter comparison

Repeat (4), this time with hyperparameters $\sigma_f = 1$ and
$\ell = 1$. Compare the results.

```{r echo=FALSE}
sigmaF2 <- 1
l <- 1
obs_x <- c(-1, -0.6, -0.2, 0.4, 0.8)
obs_y <- c(0.768, -0.044, -0.940, 0.719, -0.664)
mean_pred4  <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred4 <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data4 <- tibble(
  x_values = x_values,
  y_values = mean_pred4,
  sd = sqrt(diag(var_pred4)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data4, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = obs_x,
    y = obs_y,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (Five Training Point)", x = "x", y = "y")
```

By increasing the length scale from 0.3 to 1, the GP model becomes smoother and less sensitive to local variations in the data. This results in a posterior mean that captures broader trends rather than fitting closely to individual data points. The uncertainty bands also widen, reflecting increased uncertainty in predictions due to the smoother nature of the model.


# 2. GP Regression with kernlab

In this exercise, you will work with the daily mean temperature in
Stockholm (Tullinge) during the period January 1, 2010 – December 31,
2015. We have removed the leap year day February 29, 2012 to make things
simpler. You can read the dataset with the command:

``` r
read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master
/GaussianProcess/Code/TempTullinge.csv", header = TRUE, sep = ";")
```

Create the variable `time` which records the day number since the start
of the dataset, i.e., $$time = 1, 2, \dots, 365 \times 6 = 2190.$$

Also, create the variable `day` that records the day number since the
start of each year, i.e., $$day = 1, 2, \dots, 365, 1, 2, \dots, 365.$$

Estimating a GP on 2190 observations can take some time on slower
computers, so let us subsample the data and use only every fifth
observation. This means that your `time` and `day` variables are now:
$$time = 1, 6, 11, \dots, 2186, \quad day = 1, 6, 11, \dots, 361, 1, 6, 11, \dots, 361.$$

## (1) Familiarization and Kernel Definition

Familiarize yourself with the functions `gausspr` and `kernelMatrix` in
**kernlab**. Use `?gausspr` and read the input arguments and the output.
Also, go through the file `KernLabDemo.R` available on the course
website — you will need to understand it.

Now, define your own squared exponential kernel function (with
parameters $\ell$ and $\sigma_f$), evaluate it at the point
$x = 1, x' = 2$, and use the `kernelMatrix` function to compute the
covariance matrix $K(X, X^*)$ for the input vectors
$$X = (1, 3, 4)^T, \quad X^* = (2, 3, 4)^T.$$

```{r echo=FALSE}
rm(list = ls())
library(kernlab)
# GP Regression with kernlab
temp <- read.csv(
  "https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/TempTullinge.csv",
  header = TRUE,
  sep = ";"
)

temp_all <- temp$temp

# the hourly temp
time <- seq(1,2186,by=5)
temp_h <- temp_all[time]

# 1)
sekernel <- function(sigmaf = 1, ell = 1) 
{
  rval <- function(x, y = NULL) {
    r = sqrt(crossprod(x-y));
    return(sigmaf^2*exp(-r^2/2/ell^2))
  }
  class(rval) <- "kernel"
  return(rval)
} 

SEFunc <- sekernel(sigmaf = 1, ell = 1)
SEFunc(1,2)

X <- matrix(c(1,3,4), ncol = 1) # Simulating some data.
Xstar <- matrix(c(2,3,4), ncol = 1)

kernelMatrix(kernel = SEFunc, x = X, y = Xstar)
```

## (2) GP Model with Time as Input

Consider the following model:

$$temp = f(time) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma_n^2), \quad f \sim \mathcal{GP}(0, k(time, time')).$$

Let $\sigma_n^2$ be the residual variance from a simple quadratic
regression fit (using the `lm` function in R). Estimate the above
Gaussian process regression model using the `gausspr` function with the
squared exponential function from (1) and hyperparameters
$\sigma_f = 20$ and $\ell = 100$. Use the option `scaled = FALSE` in the
`gausspr` function; otherwise, these $\sigma_f$ and $\ell$ values are
not suitable.

Use the `predict` function in R to compute the posterior mean at every
data point in the training dataset. Make a scatterplot of the data and
superimpose the posterior mean of $f$ as a curve (use `type = "l"` in
the `plot` function). Plot also the 95% probability (pointwise) bands
for $f$.

Experiment with different values of $\sigma_f$ and $\ell$ (no need to
report this in your submission).

```{r echo=FALSE}
# 2)
# Estimating the noise variance from a third degree polynomial fit. I() is needed because, otherwise
# age^2 reduces to age in the formula, i.e. age^2 means adding the main effect and the second order
# interaction, which in this case do not exist. See ?I.
polyFit <- lm(temp_h ~  time + I(time^2) )
sigmaNoise = sd(polyFit$residuals)
plot(time,temp_h,pch = 1, cex = 0.5)

SEFunc_TT <- sekernel(sigmaf = 20, ell = 100)
GPfit <- gausspr(time,temp_h, kernel = SEFunc_TT, var = sigmaNoise^2, variance.model = TRUE,scaled=FALSE)

meanPred <- predict(GPfit, time)
lines(time, meanPred, col="red", lwd = 2)
lines(time, meanPred+1.96*predict(GPfit,time, type="sdeviation"),col="blue", lwd = 2)
lines(time, meanPred-1.96*predict(GPfit,time, type="sdeviation"),col="blue", lwd = 2)
```


## (3) Manual Implementation

Repeat the previous exercise, but now use **Algorithm 2.1** on page 19
of *Rasmussen and Williams’* book to compute the posterior mean and
variance of $f$ manually.

```{r echo=FALSE}

Kss <- kernelMatrix(kernel = SEFunc_TT, x = time, y = time)
Kxx <- Kss
Kxs <- Kss

n<-length(time)
Meanf = t(Kxs)%*%solve(Kxx + sigmaNoise^2*diag(n), temp_h)
Covf = Kss-t(Kxs)%*%solve(Kxx + sigmaNoise^2*diag(n), Kxs) # Covariance matrix of fStar.

# Probability intervals for fStar.
plot(time,temp_h,pch = 1, cex = 0.5)
lines(time, Meanf, col="red", lwd = 2)
lines(time, Meanf - 1.96*sqrt(diag(Covf)), col = "blue", lwd = 2)
lines(time, Meanf + 1.96*sqrt(diag(Covf)), col = "blue", lwd = 2)


```


## (4) GP Model with Day as Input

Consider now the following model:

$$temp = f(day) + \varepsilon, \quad \varepsilon \sim \mathcal{N}(0, \sigma_n^2), \quad f \sim \mathcal{GP}(0, k(day, day')).$$

Estimate the model using the `gausspr` function with the squared
exponential function from (1) and parameters $\sigma_f = 20$ and
$\ell = 100$ (use `scaled = FALSE`).

Superimpose the posterior mean from this model on the posterior mean
from the model in (2). Note that this plot should also have the `time`
variable on the horizontal axis.

Compare the results of both models. What are the pros and cons of each
model?

```{r echo=FALSE}

# 4) 
# every 5th day 
day <- rep(seq(1,361,by=5),6)

GPfit2 <- gausspr(day, temp_h, kernel = SEFunc_TT, var = sigmaNoise^2, variance.model = TRUE,scaled=FALSE)
meanPred_day <- predict(GPfit2, day)

plot(time,temp_h,pch = 1, cex = 0.5, main="GP with time (red) and day (blue) as input")
lines(time, meanPred, col="red", lwd = 2,)
lines(time, meanPred_day,col="blue", lwd = 2)
```
As is shown from the plot, the GP model with `day` shows clear periodic behavior, reflecting the annual cycle of temperatures, while the `time` model appears more responsive to short-term fluctuations. The `day` model captures seasonal patterns effectively, making it suitable for understanding long-term trends. However, it may overlook short-term variations that the `time` model can capture. Conversely, the `time` model is more flexible in fitting local changes but may miss the broader seasonal context. Thus, the choice between models depends on whether the focus is on capturing seasonal trends or short-term variations.


## (5) Locally Periodic Kernel Extension

Finally, implement the following extension of the squared exponential
kernel with a **periodic kernel** (also called the *locally periodic
kernel*):

$$
k(x, x') = \sigma_f^2 \exp \left(-2 \sin^2\left(\frac{\pi |x - x'|}{d}\right) / \ell_1^2 \right) \exp \left(-\frac{1}{2} \frac{|x - x'|^2}{\ell_2^2}\right).
$$

Note that we have two different length scales in the kernel.
Intuitively, $\ell_1$ controls the correlation between two days in the
same year, and $\ell_2$ controls the correlation between the same day in
different years.

Estimate the GP model using the `time` variable with this kernel and
hyperparameters $\sigma_f = 20$, $\ell_1 = 1$, $\ell_2 = 100$, and
$d = 365$. Use `gausspr` with `scaled = FALSE`, otherwise these
hyperparameters are not suitable.

Compare the fit to the previous two models (with $\sigma_f = 20$ and
$\ell = 100$). Discuss the results.

```{r echo=FALSE}
# 5)
pdckernel <- function(sigmaf = 20, ell1 = 1, ell2 = 100, d = 365) 
{
  rval <- function(x, y = NULL) {
    r = sqrt(crossprod(x-y))
    return(sigmaf^2*exp(-2*sin(pi * r / d)^2/ell1^2)* exp(-1/2 * r^2/ell2^2) )
  }
  class(rval) <- "kernel"
  return(rval)
}

pdcFunc_TT <- pdckernel(sigmaf = 20, ell = 1, ell2 = 100, d = 365)
GPfit3 <- gausspr(time,temp_h, kernel = pdcFunc_TT, var = sigmaNoise^2, variance.model = TRUE,scaled=FALSE)
meanPred_pdc <- predict(GPfit3, time)

plot(time,temp_h,pch = 1, cex = 0.5, main="GP with time (red), day (blue) and locally periodic (green) as input")
lines(time, meanPred_pdc, col="green", lwd = 2 )
lines(time, meanPred, col="red", lwd = 2)
lines(time, meanPred_day,col="blue", lwd = 2)
```
The locally periodic kernel effectively captures both the seasonal patterns and short-term fluctuations in the temperature data. The posterior mean from this model (green line) shows clear periodic behavior similar to the `day` model (blue line), while also adapting to local variations like the `time` model (red line). This indicates that the locally periodic kernel successfully integrates the strengths of both previous models, providing a more comprehensive fit to the data. The choice of hyperparameters allows for fine-tuning the balance between capturing long-term trends and short-term changes, making this approach particularly versatile for modeling time series data with inherent periodicity.


# 3. GP Classification with kernlab

Download the **banknote fraud** data:

``` r
data <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master
/GaussianProcess/Code/banknoteFraud.csv", header = FALSE, sep = ",")

names(data) <- c("varWave", "skewWave", "kurtWave", "entropyWave", "fraud")
data[, 5] <- as.factor(data[, 5])
```

You can read more about this dataset online. Choose **1000
observations** as training data using the following command (i.e., use
the vector `SelectTraining` to subset the training observations):

``` r
set.seed(111)
SelectTraining <- sample(1:dim(data)[1], size = 1000, replace = FALSE)
```

## (1) GP Classification Model with Two Covariates

Use the R package **kernlab** to fit a **Gaussian process
classification** model for the response variable `fraud` on the training
data. Use the **default kernel** and **default hyperparameters**. Start
by using only the covariates `varWave` and `skewWave` in the model.

Plot contours of the **prediction probabilities** over a suitable grid
of values for `varWave` and `skewWave`. Overlay the training data where
`fraud = 1` (as **blue points**) and `fraud = 0` (as **red points**).
You can reuse code from the file `KernLabDemo.R` available on the course
website.

Compute the **confusion matrix** for the classifier and its
**accuracy**.
```{r echo=FALSE}
rm(list=ls())
data <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/banknoteFraud.csv", header=FALSE, sep=",")
names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud")
data[,5] <- as.factor(data[,5])


set.seed(111); 
SelectTraining <- sample(1:dim(data)[1], size = 1000,replace = FALSE)
train <-data[SelectTraining,]
test <-data[-SelectTraining,]
```

```{r echo=FALSE}
library(kernlab)
GPfit <- gausspr(fraud ~  varWave + skewWave, data=train)
pred <- predict(GPfit,train)
x1 <- seq(min(data$varWave),max(data$varWave),length=100)
x2 <- seq(min(data$skewWave),max(data$skewWave),length=100)

gridPoints <- matrix(NA,100*100,2)
for(i in 1:100)
  for(j in 1:100)
    gridPoints[(i-1)*100+j,] <- c(x1[i],x2[j])

gridPoints <- data.frame(gridPoints)
names(gridPoints) <- names(train)[1:2]
probPreds <- predict(GPfit, gridPoints, type="probabilities")

```


```{r echo=FALSE}

table(pred,train$fraud)
# Accuracy
acc <- function(tab) sum(diag(tab))/sum(tab)
acc(table(pred,train$fraud))

# Plotting for Prob
z <- matrix(probPreds[,2],nrow = 100, ncol = 100, byrow = FALSE)
contour(x1,x2,z, 20, xlab = "varWave", ylab = "skewWave", main = 'Prob(Fraud) - Fraud is blue')
points(train[train[,5]==1,1],train[train[,5]==1,2],col="blue",pch = 20)
points(train[train[,5]==0,1],train[train[,5]==0,2],col="red", pch = 20)

```


## (2) Test Set Prediction

Using the model estimated in (1), make predictions for the **test set**
(the observations not included in `SelectTraining`). Compute the
**accuracy** on this test set.

```{r echo=FALSE}

pred_test <- predict(GPfit,test)
table(pred_test,test$fraud)
# Accuracy
acc(table(pred_test,test$fraud))
```

## (3) Model with All Covariates

Train a new model using **all four covariates**: `varWave`, `skewWave`,
`kurtWave`, and `entropyWave`. Make predictions on the test set and
compare the **accuracy** to the model with only two covariates.

Discuss how the inclusion of more covariates affects the model’s
classification performance.

```{r echo=FALSE}
GPfit2 <- gausspr(fraud ~  varWave + skewWave + kurtWave + entropyWave, data=train)
pred_test2 <- predict(GPfit2,test)
table(pred_test2,test$fraud)
# Accuracy
acc(table(pred_test2,test$fraud))
```

The inclusion of all four covariates in the GP classification model leads to a noticeable improvement in classification accuracy on the test set compared to the model with only two covariates. By incorporating `kurtWave` and `entropyWave`, the model gains access to additional information that helps distinguish between fraudulent and non-fraudulent banknotes more effectively. This enhanced feature set allows the GP model to capture more complex patterns in the data, resulting in better predictive performance. Overall, using all available covariates provides a more comprehensive understanding of the underlying factors contributing to fraud, thereby improving the model's ability to classify new observations accurately.


# Appendix 

```{r eval=FALSE} 
# 1 
SquaredExpKernel <- function(x1, x2, sigmaF = 1, l = 1) {
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA, n1, n2)
  for (i in 1:n2) {
    K[, i] <- sigmaF^2 * exp(-0.5 * ((x1 - x2[i]) / l)^2)
  }
  return(K)
}

posteriorGP <- function (x, y, xstar, sig, k, ...) {

  K <- k(x, x, ...)
  Kstar <- k(x, xstar, ...)
  Kss <- k(xstar, xstar, ...)
  # predictive mean
  L <- chol(K + sig^2 * diag(nrow(K)))  # add noise
  # L in the algorithm is a lower triangular matrix,
  # whereas the R function returns an upper triangular matrix
  L <- t(L)
  
  alpha <- solve(t(L), solve(L, y))
  mean <- t(Kstar) %*% alpha
  
  # predictive var
  v <- solve (L, Kstar)
  var <-  Kss - t(v) %*% v
  
  res = list(mean = mean, var = var)
  return(res)
}

sigmaF <- 1
l <- 0.3

library(tibble)
library(ggplot2)
#tibble
x_values <- seq(-1, 1, length.out = 5000)
mean_pred  <- posteriorGP(0.4, 0.719, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred <- posteriorGP(0.4, 0.719, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data <- tibble(
  x_values = x_values,
  y_values = mean_pred,
  sd = sqrt(diag(var_pred)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = 0.4,
    y = 0.719,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (One Training Point)", x = "x", y = "y")

obs_x <- c(0.4, -0.6)
obs_y <- c(0.719, -0.044)

#tibble
mean_pred2  <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred2 <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data2 <- tibble(
  x_values = x_values,
  y_values = mean_pred2,
  sd = sqrt(diag(var_pred2)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data2, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = obs_x,
    y = obs_y,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (Two Training Point)", x = "x", y = "y")

obs_x <- c(-1, -0.6, -0.2, 0.4, 0.8)
obs_y <- c(0.768, -0.044, -0.940, 0.719, -0.664)
mean_pred3  <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred3 <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data3 <- tibble(
  x_values = x_values,
  y_values = mean_pred3,
  sd = sqrt(diag(var_pred3)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data3, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = obs_x,
    y = obs_y,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (Five Training Point)", x = "x", y = "y")

sigmaF2 <- 1
l <- 1
obs_x <- c(-1, -0.6, -0.2, 0.4, 0.8)
obs_y <- c(0.768, -0.044, -0.940, 0.719, -0.664)
mean_pred4  <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$mean
var_pred4 <- posteriorGP(obs_x, obs_y, x_values, 0.1, k = SquaredExpKernel, sigmaF, l)$var

my_data4 <- tibble(
  x_values = x_values,
  y_values = mean_pred4,
  sd = sqrt(diag(var_pred4)),
  upp = y_values + 1.96 * sd,
  low = y_values - 1.96 * sd
)

ggplot(data = my_data4, aes(x = x_values, y = y_values)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = low, ymax = upp),
              fill = "skyblue",
              alpha = 0.3) +
  annotate(
    "point",
    x = obs_x,
    y = obs_y,
    color = "red",
    size = 3
  ) +
  theme_minimal() +
  labs(title = "Gaussian Process Regression (Five Training Point)", x = "x", y = "y")
```

```{r eval=FALSE}
# 2
rm(list = ls())
library(kernlab)
# GP Regression with kernlab
temp <- read.csv(
  "https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/TempTullinge.csv",
  header = TRUE,
  sep = ";"
)

temp_all <- temp$temp

# the hourly temp
time <- seq(1,2186,by=5)
temp_h <- temp_all[time]

# 1)
sekernel <- function(sigmaf = 1, ell = 1) 
{
  rval <- function(x, y = NULL) {
    r = sqrt(crossprod(x-y));
    return(sigmaf^2*exp(-r^2/2/ell^2))
  }
  class(rval) <- "kernel"
  return(rval)
} 

SEFunc <- sekernel(sigmaf = 1, ell = 1)
SEFunc(1,2)

X <- matrix(c(1,3,4), ncol = 1) # Simulating some data.
Xstar <- matrix(c(2,3,4), ncol = 1)

kernelMatrix(kernel = SEFunc, x = X, y = Xstar)

# 2)
# Estimating the noise variance from a third degree polynomial fit. I() is needed because, otherwise
# age^2 reduces to age in the formula, i.e. age^2 means adding the main effect and the second order
# interaction, which in this case do not exist. See ?I.
polyFit <- lm(temp_h ~  time + I(time^2) )
sigmaNoise = sd(polyFit$residuals)
plot(time,temp_h,pch = 1, cex = 0.5)

SEFunc_TT <- sekernel(sigmaf = 20, ell = 100)
GPfit <- gausspr(time,temp_h, kernel = SEFunc_TT, var = sigmaNoise^2, variance.model = TRUE,scaled=FALSE)

meanPred <- predict(GPfit, time)
lines(time, meanPred, col="red", lwd = 2)
lines(time, meanPred+1.96*predict(GPfit,time, type="sdeviation"),col="blue", lwd = 2)
lines(time, meanPred-1.96*predict(GPfit,time, type="sdeviation"),col="blue", lwd = 2)

Kss <- kernelMatrix(kernel = SEFunc_TT, x = time, y = time)
Kxx <- Kss
Kxs <- Kss

n<-length(time)
Meanf = t(Kxs)%*%solve(Kxx + sigmaNoise^2*diag(n), temp_h)
Covf = Kss-t(Kxs)%*%solve(Kxx + sigmaNoise^2*diag(n), Kxs) # Covariance matrix of fStar.

# Probability intervals for fStar.
plot(time,temp_h,pch = 1, cex = 0.5)
lines(time, Meanf, col="red", lwd = 2)
lines(time, Meanf - 1.96*sqrt(diag(Covf)), col = "blue", lwd = 2)
lines(time, Meanf + 1.96*sqrt(diag(Covf)), col = "blue", lwd = 2)

# 4) 
# every 5th day 
day <- rep(seq(1,361,by=5),6)

GPfit2 <- gausspr(day, temp_h, kernel = SEFunc_TT, var = sigmaNoise^2, variance.model = TRUE,scaled=FALSE)
meanPred_day <- predict(GPfit2, day)

plot(time,temp_h,pch = 1, cex = 0.5, main="GP with time (red) and day (blue) as input")
lines(time, meanPred, col="red", lwd = 2,)
lines(time, meanPred_day,col="blue", lwd = 2)


# 5)
pdckernel <- function(sigmaf = 20, ell1 = 1, ell2 = 100, d = 365) 
{
  rval <- function(x, y = NULL) {
    r = sqrt(crossprod(x-y))
    return(sigmaf^2*exp(-2*sin(pi * r / d)^2/ell1^2)* exp(-1/2 * r^2/ell2^2) )
  }
  class(rval) <- "kernel"
  return(rval)
}

pdcFunc_TT <- pdckernel(sigmaf = 20, ell = 1, ell2 = 100, d = 365)
GPfit3 <- gausspr(time,temp_h, kernel = pdcFunc_TT, var = sigmaNoise^2, variance.model = TRUE,scaled=FALSE)
meanPred_pdc <- predict(GPfit3, time)

plot(time,temp_h,pch = 1, cex = 0.5, main="GP with time (red), day (blue) and locally periodic (green) as input")
lines(time, meanPred_pdc, col="green", lwd = 2 )
lines(time, meanPred, col="red", lwd = 2)
lines(time, meanPred_day,col="blue", lwd = 2)
```

```{r eval=FALSE}
# 3
rm(list=ls())
data <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/banknoteFraud.csv", header=FALSE, sep=",")
names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud")
data[,5] <- as.factor(data[,5])


set.seed(111); 
SelectTraining <- sample(1:dim(data)[1], size = 1000,replace = FALSE)
train <-data[SelectTraining,]
test <-data[-SelectTraining,]

library(kernlab)
GPfit <- gausspr(fraud ~  varWave + skewWave, data=train)
pred <- predict(GPfit,train)
x1 <- seq(min(data$varWave),max(data$varWave),length=100)
x2 <- seq(min(data$skewWave),max(data$skewWave),length=100)

gridPoints <- matrix(NA,100*100,2)
for(i in 1:100)
  for(j in 1:100)
    gridPoints[(i-1)*100+j,] <- c(x1[i],x2[j])

gridPoints <- data.frame(gridPoints)
names(gridPoints) <- names(train)[1:2]
probPreds <- predict(GPfit, gridPoints, type="probabilities")

table(pred,train$fraud)
# Accuracy
acc <- function(tab) sum(diag(tab))/sum(tab)
acc(table(pred,train$fraud))

# Plotting for Prob
z <- matrix(probPreds[,2],nrow = 100, ncol = 100, byrow = FALSE)
contour(x1,x2,z, 20, xlab = "varWave", ylab = "skewWave", main = 'Prob(Fraud) - Fraud is blue')
points(train[train[,5]==1,1],train[train[,5]==1,2],col="blue",pch = 20)
points(train[train[,5]==0,1],train[train[,5]==0,2],col="red", pch = 20)


pred_test <- predict(GPfit,test)
table(pred_test,test$fraud)
# Accuracy
acc(table(pred_test,test$fraud))

GPfit2 <- gausspr(fraud ~  varWave + skewWave + kurtWave + entropyWave, data=train)
pred_test2 <- predict(GPfit2,test)
table(pred_test2,test$fraud)
# Accuracy
acc(table(pred_test2,test$fraud))
```

